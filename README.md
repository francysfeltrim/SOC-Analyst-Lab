# üõ°Ô∏è Building a SOC Home Lab: Detection & Response Project

Este projeto documenta a constru√ß√£o de um Laborat√≥rio de Security Operations Center (SOC) para simular ataques reais e praticar Defesa Cibern√©tica (Blue Team). O objetivo √© implementar uma stack completa de monitoramento (ELK), ingerir logs de endpoints e desenvolver habilidades de detec√ß√£o e resposta a incidentes.

## üìå Fase 1: Arquitetura e Infraestrutura 

### 1. Arquitetura L√≥gica
Antes do deploy, foi desenhada a topologia da rede para entender o fluxo de dados e posicionamento dos sensores. O laborat√≥rio simula um ambiente corporativo real contendo:

![Topologia de Rede](images/00-network-topology.png)
*Diagrama da arquitetura do laborat√≥rio desenhado durante o planejamento.*
* **VPC (Virtual Private Cloud):** Para isolamento da rede.
* **SIEM (ELK Stack):** O cora√ß√£o do monitoramento.
* **Endpoints (Windows/Linux):** Alvos que ser√£o monitorados e atacados.
* **C2 Server (Command & Control):** Para simular o atacante externo.

### 2. Provisionamento de Infraestrutura (IaaS)
Utilizei a **Vultr** como provedor de nuvem. A escolha foi baseada na capacidade de configurar redes privadas personalizadas e baixo custo.

**Configura√ß√µes do Servidor SIEM:**
* **OS:** Ubuntu 22.04 LTS (Estabilidade e suporte da comunidade).
* **Specs:** 2 vCPU, 8GB RAM (Dimensionado para suportar a JVM do Elasticsearch).
* **Regi√£o:** Toronto (Para garantir compatibilidade de lat√™ncia com a VPC).

![Status da VM e Specs](images/01-infrastructure-deploy.png)
*Deploy da inst√¢ncia Ubuntu Server conclu√≠do e rodando.*

### 3. Configura√ß√£o de Rede e Seguran√ßa (Hardening)
A seguran√ßa do pr√≥prio laborat√≥rio foi uma prioridade. N√£o expus o servidor diretamente sem prote√ß√µes.

* **VPC Network:** Criei uma rede privada isolada para que os agentes (futuros servidores Windows/Linux) se comuniquem com o SIEM sem trafegar logs sens√≠veis pela internet p√∫blica.
* **Firewall na Nuvem:** Implementei um *Firewall Group* na Vultr seguindo o princ√≠pio do menor privil√©gio.
    * **Porta 22 (SSH):** Restrita apenas ao meu IP atual de gerenciamento.
    * **Porta 9200 (Elasticsearch):** Restrita ao meu IP e √† rede interna da VPC.

![Regras de Firewall](images/02-firewall-hardening.png)
*Aplica√ß√£o de regras de firewall restringindo acesso SSH e TCP/9200.*

### 4. Instala√ß√£o e Configura√ß√£o do Elasticsearch
Realizei a instala√ß√£o manual do Elasticsearch via reposit√≥rio APT oficial, garantindo a vers√£o mais recente e segura.

**Credenciais e Seguran√ßa Inicial:**
Durante a instala√ß√£o, o Elasticsearch gerou automaticamente os tokens de seguran√ßa e a senha do superusu√°rio, garantindo que o banco de dados j√° nas√ßa autenticado.

![Instala√ß√£o e Senhas](images/03-elasticsearch-install.png)
*Output de seguran√ßa p√≥s-instala√ß√£o com credenciais geradas.*

**Ajustes de Configura√ß√£o (`elasticsearch.yml`):**
Para permitir que o servidor receba conex√µes externas (do meu laptop de analista e dos agentes), precisei alterar as configura√ß√µes de *binding* de rede, saindo do padr√£o `localhost`.

![Configura√ß√£o Nano](images/04-config-yaml.png)
*Edi√ß√£o do arquivo YAML definindo o host de rede p√∫blico e a porta padr√£o 9200.*

**Status do Servi√ßo:**
Ap√≥s a configura√ß√£o e recarga dos *daemons*, o servi√ßo subiu com sucesso e est√° ativo.

![Status Active](images/05-service-active.png)
*Valida√ß√£o do servi√ßo rodando via SystemD.*

---

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting)

Durante essa fase inicial, enfrentei desafios t√©cnicos reais de ambiente Cloud que exigiram adapta√ß√£o:

**1. Cota de Recursos na Cloud (Resource Quotas)**
Ao tentar provisionar a m√°quina de 16GB, fui bloqueada pela pol√≠tica de cota para novas contas ("Monthly Fee Limit Reached").

![Erro de Cota](images/99-troubleshooting-quota.png)
*Erro de limite de provisionamento encontrado.*

* **Solu√ß√£o:** Realizei o *downsizing* estrat√©gico para uma inst√¢ncia de 8GB RAM e otimizei o servidor. Isso permitiu continuar o laborat√≥rio dentro do or√ßamento e das limita√ß√µes da conta, sem perder funcionalidade cr√≠tica.

**3. IP Din√¢mico (CGNAT)**
Meu provedor de internet altera o IP frequentemente, o que bloqueava meu acesso √†s regras restritas do Firewall.
* **Solu√ß√£o:** Aprendi a monitorar meu IP p√∫blico e atualizar as regras de *Ingress* dinamicamente. Para testes de conectividade r√°pida, gerenciei o risco temporariamente via regras "Anywhere" combinadas com a autentica√ß√£o forte nativa do Elastic.

---

## üìå Fase 2: Visualiza√ß√£o de Dados com Kibana 

Com o backend de logs (Elasticsearch) funcional, o pr√≥ximo passo foi implementar o **Kibana**, a interface gr√°fica que permitir√° a visualiza√ß√£o dos dados, cria√ß√£o de dashboards e investiga√ß√£o de alertas de seguran√ßa.

### 1. Instala√ß√£o e Exposi√ß√£o do Servi√ßo
O Kibana foi instalado no mesmo servidor Ubuntu. Diferente do Elasticsearch (Backend), o Kibana precisa ser acess√≠vel via navegador.

**Configura√ß√£o de Rede (`kibana.yml`):**
Editei o arquivo de configura√ß√£o para alterar o `server.host`. Por padr√£o, ele vem travado em `localhost`. Configurei para ouvir no IP p√∫blico do servidor, permitindo o acesso remoto √† interface web na porta padr√£o `5601`.

![Configura√ß√£o Kibana](images/06-kibana-config-yaml.png)
*Ajuste do binding de rede para permitir acesso externo √† interface web.*

**Valida√ß√£o do Servi√ßo:**
Ap√≥s a configura√ß√£o, o servi√ßo foi iniciado e verificado via SystemD para garantir que n√£o houvesse erros de *bootstrap*.

![Status do Servi√ßo](images/07-kibana-service-status.png)
*Servi√ßo do Kibana ativo e rodando (Active/Running).*

### 2. Conex√£o Segura (Enrollment Token)
Para conectar o Kibana ao Elasticsearch de forma segura, utilizei o mecanismo de **Enrollment Tokens**. Isso garante que a comunica√ß√£o entre a interface e o banco de dados seja autenticada e criptografada, prevenindo intercepta√ß√£o de dados.

![Gera√ß√£o de Token](images/08-security-enrollment.png)
*Gera√ß√£o do token de inscri√ß√£o para pareamento seguro entre Kibana e Elasticsearch.*

### 3. Acesso e Configura√ß√£o de Criptografia (Keystore)
Ap√≥s o login inicial, obtive acesso √† interface central do Elastic ("Welcome Home"), confirmando que a stack ELK estava operacional.

![Elastic Home](images/10-elastic-home-welcome.png)
*Acesso bem-sucedido √† interface web do Elastic Stack.*

**Troubleshooting: Erro de Permiss√µes e Chaves de Criptografia**
Ao navegar para a aba de **Security > Alerts**, deparei-me com um erro de sistema: *"Detection engine permissions required"*.
Investigando a documenta√ß√£o, identifiquei que o erro n√£o era de permiss√µes de utilizador, mas sim a aus√™ncia de chaves de criptografia no *Keystore* do Kibana, necess√°rias para armazenar regras de alerta de forma segura.

![Erro Encryption](images/97-troubleshooting-encryption-error.png)
*Erro apresentado devido √† falta de chaves de criptografia persistentes.*

**Solu√ß√£o Aplicada:**
1.  Gerei novas chaves de criptografia via CLI (`kibana-encryption-keys generate`).
2.  Adicionei as chaves manualmente ao cofre seguro do Kibana (`kibana-keystore add`).
3.  Reiniciei o servi√ßo para aplicar as altera√ß√µes.

![Gerando Chaves](images/11-generating-encryption-keys.png)
*Gera√ß√£o e inser√ß√£o das chaves de seguran√ßa no Keystore.*

**Resultado Final:**
O painel de Alertas carregou com sucesso, pronto para receber dete√ß√µes de seguran√ßa.

![Alerts Corrigido](images/12-alerts-dashboard-fixed.png)
*Painel de Security Alerts totalmente operacional ap√≥s a corre√ß√£o.*

---

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting)

**1. Bloqueio de Firewall e Portas Reservadas**
Ao tentar acessar a interface web (`http://IP:5601`), recebi erros de *Connection Timed Out*. Diagnostiquei que o Firewall da Cloud (Vultr) estava bloqueando a porta 5601.
Ao tentar liberar o tr√°fego TCP, cometi um erro ao definir o range de portas iniciando em `0` (`0-65535`), o que foi rejeitado pela plataforma.

![Erro Firewall](images/98-troubleshooting-firewall-error.png)
*Erro ao tentar configurar regra de firewall com porta inv√°lida (0).*

* **Solu√ß√£o:** Ajustei a regra para um range v√°lido (`1-65535`) e configurei o acesso tempor√°rio para `Anywhere` (0.0.0.0/0) para fins de teste de conectividade, liberando o acesso ao painel do Kibana.

![Firewall Corrigido](images/09-firewall-fixed-kibana.png)
*Regra de firewall corrigida permitindo tr√°fego TCP na porta do Kibana.*

---

## üìå Fase 3: V√≠tima e Hardening 

Para simular um cen√°rio real de ataque, provisionei um servidor Windows Server 2022 exposto √† internet. Este servidor atuar√° como o *endpoint* monitorado e alvo das simula√ß√µes de ataque.

### 1. Arquitetura de Seguran√ßa (Isolamento)
Diferente dos componentes do SIEM (ELK), decidi **n√£o** conectar o servidor Windows √† VPC (Rede Privada).
* **Objetivo:** Garantir isolamento total (Network Segmentation). Caso o servidor Windows seja comprometido por um atacante real (o que √© esperado, dado que exporemos RDP), o atacante n√£o ter√° rota de rede lateral para alcan√ßar meu servidor de logs (Ubuntu/Elasticsearch).

![Specs Windows](images/13-windows-isolation-specs.png)
*Provisionamento do Windows Server 2022 fora da VPC para quarentena de rede.*

### 2. Acesso e Configura√ß√£o Inicial
O acesso inicial foi realizado via Console VNC (NoVNC) provido pela plataforma de nuvem para garantir que o sistema operacional completou o *boot* corretamente antes de expor servi√ßos de rede.

![Console Boot](images/14-windows-console-boot.png)
*Boot inicial e login administrativo via Console Web.*

### 3. Exposi√ß√£o Controlada (RDP)
Habilitei o protocolo RDP (Remote Desktop Protocol - Porta 3389) para administra√ß√£o remota.
* **Risco Aceito:** Manter o RDP exposto na internet √© uma vulnerabilidade cr√≠tica comum. Neste laborat√≥rio, isso √© intencional para gerar logs de *Brute Force* reais que ser√£o capturados e analisados pelo SIEM nas pr√≥ximas etapas.

![Acesso RDP](images/15-windows-rdp-access.png)
*Conex√£o remota bem-sucedida provando a acessibilidade p√∫blica do alvo.*

---
## üìå Fase 4: Ingest√£o de Dados e Fleet Server 

Com a infraestrutura do SIEM (ELK) e da V√≠tima (Windows) prontas, a pr√≥xima etapa foi conect√°-los. Para isso, utilizei a arquitetura **Elastic Fleet**, que centraliza o gerenciamento de todos os agentes de coleta.

Esta fase exigiu a cria√ß√£o de uma terceira VM Linux (`MyDFIR-Fleet-Server`) para atuar como o "Gerente" dos agentes, seguindo as boas pr√°ticas de separa√ß√£o de fun√ß√µes.

![Specs Fleet Server](images/16-fleet-server-deploy-specs.png)
*Provisionamento da VM dedicada para o Fleet Server.*

---

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting Avan√ßado)

Esta foi a fase mais complexa do projeto at√© o momento, apresentando m√∫ltiplos pontos de falha que exigiram diagn√≥stico em diferentes camadas (Rede, Aplica√ß√£o e Configura√ß√£o).

#### 1. Troubleshooting (Linux): Firewall e Conectividade
Ao tentar instalar o Fleet Server (agente Linux), a instala√ß√£o falhou com erros de `i/o timeout`.

![Erro de Conex√£o](images/17-troubleshoot-linux-firewall-error.png)
*Log de erro indicando que o Fleet Server n√£o conseguia se comunicar com o Elasticsearch na porta 9200.*

* **Diagn√≥stico:** O Firewall Group da Vultr, configurado para aceitar conex√µes apenas do "Meu IP", estava bloqueando a comunica√ß√£o interna entre os servidores (Fleet n√£o conseguia falar com ELK).
* **Solu√ß√£o:** Alterei a regra de firewall para `Anywhere (0.0.0.0/0)` para o range `1-65535`, permitindo a comunica√ß√£o interna necess√°ria para o laborat√≥rio. Isso resolveu o bloqueio das portas **9200** (Elastic) e **8220** (Fleet).

![Firewall Fix](images/18-vultr-firewall-fix.png)
*Ajuste nas regras de firewall para permitir a comunica√ß√£o interna do lab.*

Ap√≥s a corre√ß√£o do firewall, a instala√ß√£o do Fleet Server no Linux foi conclu√≠da com sucesso.

![Sucesso Linux](images/19-linux-agent-install-success.png)
*Instala√ß√£o do agente Fleet Server bem-sucedida.*

#### 2. Troubleshooting (Windows): Instala√ß√£o do Agente
A implanta√ß√£o do agente no Windows Server apresentou tr√™s erros em sequ√™ncia:
1.  **Erro de PowerShell:** O comando copiado do Kibana era longo e quebrava linhas, fazendo o PowerShell execut√°-lo incorretamente.
2.  **Erro de Caminho:** O PowerShell n√£o encontrava o `elastic-agent.exe` pois eu n√£o estava no diret√≥rio correto.
3.  **Erro de Loop (`:443`):** O agente instalava, mas entrava em loop infinito de conex√£o.

* **Solu√ß√£o (Comando Final):** Resolvi todos os problemas de uma vez construindo um comando de instala√ß√£o manual e robusto.
    1.  Naveguei para o diret√≥rio correto (`cd elastic-agent...`).
    2.  Usei o IP e a porta **correta** (`:8220`).
    3.  Adicionei a flag `--force` para sobrescrever a instala√ß√£o anterior falha.

![Sucesso Windows](images/20-windows-agent-install-success.png)
*Comando final no PowerShell (com `cd` e `--force`) que resultou na instala√ß√£o bem-sucedida.*

#### 3. Troubleshooting (Kibana): O Loop "Updating"
Ap√≥s a instala√ß√£o, o agente Windows ficou preso no status "Updating".
* **Diagn√≥stico:** Ao inspecionar a mensagem de erro no Kibana, notei que o agente tentava se comunicar na porta `:443`, apesar de eu ter for√ßado a instala√ß√£o na `:8220`. A **Pol√≠tica de Agente** (Agent Policy) no Kibana estava configurada com a URL errada, sobrescrevendo minha instala√ß√£o manual.

![Erro de Pol√≠tica](images/21-troubleshoot-kibana-policy-error.png)
*Configura√ß√£o do Fleet Server no Kibana apontando para a porta errada (443).*

* **Solu√ß√£o Definitiva:** Editei as configura√ß√µes do Fleet Server diretamente no Kibana, corrigindo a URL global para `https://[IP_DO_FLEET]:8220`. Ap√≥s reiniciar o servi√ßo no Windows (`Stop-Service/Start-Service`), o agente recebeu a pol√≠tica correta.

### 4. Valida√ß√£o Final da Infraestrutura
Com todas as corre√ß√µes aplicadas, ambos os agentes (Linux Fleet Server e Windows V√≠tima) reportaram status **Healthy** (Saud√°vel), confirmando que a infraestrutura de coleta de logs est√° 100% operacional.

![Dashboard Fleet](images/22-fleet-dashboard-all-healthy.png)
*Vis√£o final do painel Fleet com todos os agentes online e saud√°veis.*

---
## üìå Fase 5: Enriquecimento de Logs com Sysmon 

Com os agentes online, o pr√≥ximo passo foi enriquecer a qualidade dos dados coletados. O Elastic Agent coleta os logs de seguran√ßa padr√£o do Windows, mas para uma detec√ß√£o de amea√ßas eficaz (Threat Hunting), √© necess√°ria uma telemetria mais profunda.

Para isso, instalei o **Sysmon (System Monitor)** da Microsoft, a ferramenta padr√£o da ind√∫stria para monitoramento avan√ßado de *endpoints*.

### 1. Instala√ß√£o e Configura√ß√£o
A instala√ß√£o foi realizada no servidor Windows Server (V√≠tima). O ponto crucial foi n√£o instalar o Sysmon com as configura√ß√µes padr√£o (que s√£o muito "barulhentas").

Utilizei uma configura√ß√£o personalizada (`.xml`) baseada no projeto *SwiftOnSecurity*, que √© um padr√£o de mercado. Este arquivo filtra eventos de sistema irrelevantes e foca no que √© importante para a seguran√ßa, otimizando a ingest√£o de dados no SIEM.

![Instala√ß√£o Sysmon](images/23-sysmon-install-powershell.png)
*Instala√ß√£o do Sysmon via PowerShell (Admin), aplicando o arquivo de configura√ß√£o .xml.*

### 2. Valida√ß√£o Local
Ap√≥s a instala√ß√£o, validei que o Sysmon estava operacional na pr√≥pria m√°quina antes de tentar configur√°-lo no SIEM.

**1. Verifica√ß√£o do Servi√ßo:**
Confirmei que o servi√ßo `Sysmon64` foi instalado e estava em execu√ß√£o (`Running`) no `services.msc`.

![Servi√ßo Sysmon](images/24-sysmon-service-running.png)
*Servi√ßo Sysmon64 ativo e rodando em segundo plano.*

**2. Verifica√ß√£o dos Logs:**
Confirmei no **Visualizador de Eventos (Event Viewer)** que os logs estavam sendo gerados. Isso prova que o Sysmon est√° monitorando ativamente o sistema.

![Logs Sysmon Locais](images/25-sysmon-local-event-viewer.png)
*Logs operacionais do Sysmon (ex: Event ID 3 - Network connection) sendo gerados localmente.*

---
## üìå Fase 6: Ingest√£o de Logs no SIEM 

Com o Sysmon a gerar logs localmente, o passo final da infraestrutura foi configurar o *Elastic Agent* para ler esses arquivos e envi√°-los para o Elasticsearch.

### 1. Integra√ß√£o de Fontes de Dados (Data Ingestion)
No Kibana, configurei a pol√≠tica do agente Windows para incluir duas novas integra√ß√µes de **"Custom Windows Event Logs"**. Isso instrui o agente a ler canais espec√≠ficos do Windows Event Viewer.

**Canais Configurados:**
* **Sysmon:** `Microsoft-Windows-Sysmon/Operational` (Foco em cria√ß√£o de processos e rede).
* **Windows Defender:** `Microsoft-Windows-Windows Defender/Operational` (Foco em dete√ß√£o de malware).

![Config Sysmon](images/26-integration-sysmon-config.png)
*Configura√ß√£o do canal de ingest√£o para logs do Sysmon.*

![Config Defender](images/27-integration-defender-config.png)
*Configura√ß√£o do canal de ingest√£o para logs do Windows Defender.*

### 2. Valida√ß√£o de Recebimento (Data Discovery)
Ap√≥s aplicar a pol√≠tica, aguardei a propaga√ß√£o para o agente e validei o recebimento dos dados na aba **Discover** do Kibana.

Realizei testes gerando atividade no servidor (como reiniciar servi√ßos de seguran√ßa) para confirmar que os logs estavam a chegar quase em tempo real.

**Resultado:**
Os logs do Sysmon (ex: *Process Create*, Event ID 1) e do Defender come√ßaram a ser indexados corretamente pelo SIEM.

![Logs Sysmon](images/29-discover-sysmon-logs.png)
*Prova de ingest√£o: Log detalhado do Sysmon visualizado no Kibana.*

![Volume de Dados](images/30-discover-event-volume.png)
*Gr√°fico de volume de eventos confirmando o fluxo cont√≠nuo de dados entre a V√≠tima e o SIEM.*

---
## üìå Fase 7: Cria√ß√£o de Honeypot SSH e An√°lise de Ataques (Dia 12)

O objetivo desta fase era provisionar um servidor Linux exposto √† internet para atuar como "isca" (Honeypot) e capturar tentativas reais de ataque SSH (Brute Force).

### 1. Otimiza√ß√£o de Recursos (Engenharia)
Durante o provisionamento de uma quarta inst√¢ncia (Linux Target), atingi o limite de cota da conta de nuvem (Cloud Resource Quotas).

![Decis√£o de Recurso](images/31-resource-optimization-decision.png)
*Limite de inst√¢ncias atingido durante a tentativa de scale-out.*

* **Solu√ß√£o Arquitetural:** Em vez de solicitar aumento de cota (o que geraria custos), optei por reutilizar o servidor `MyDFIR-Fleet-Server`. Como ele j√° √© um servidor Linux Ubuntu exposto √† internet (necess√°rio para os agentes remotos), ele serve perfeitamente como alvo duplo: **Gerenciador de Agentes** e **Honeypot SSH**.

### 2. An√°lise de Logs de Autentica√ß√£o (`auth.log`)
Acessando o servidor via SSH, analisei os logs de autentica√ß√£o localizados em `/var/log/auth.log`.
Em pouco mais de 24 horas de exposi√ß√£o √† internet, o servidor registrou centenas de tentativas de acesso n√£o autorizado vindas de m√∫ltiplos endere√ßos IP globais.

**Evid√™ncias de Ataque:**
Os logs mostram bots tentando adivinhar senhas para usu√°rios comuns (`root`, `admin`) e servi√ßos espec√≠ficos (`git`, `composer`, `squid`).

![Logs de Ataque](images/32-ssh-bruteforce-evidence.png)
*Live logs demonstrando tentativas massivas de Brute Force contra o servidor exposto.*

---
## üìå Fase 8: Ingest√£o de Logs Linux e Monitoramento SSH (Dia 13)

Com o servidor Linux ("Honeypot") sob ataque constante, configurei o agente para coletar esses logs e envi√°-los para o SIEM, permitindo an√°lise centralizada.

### 1. Configura√ß√£o da Integra√ß√£o de Sistema
Como o *Fleet Server* j√° possu√≠a o Elastic Agent instalado, precisei apenas validar a pol√≠tica de agentes. Confirmei que a integra√ß√£o **System** estava ativa e configurada para ler os logs de autentica√ß√£o do sistema operacional.

* **Caminho do Log:** `/var/log/auth.log` (Padr√£o Ubuntu/Debian).
* **Dataset:** `system.auth`.

![Configura√ß√£o Linux](images/33-linux-system-integration-config.png)
*Configura√ß√£o da pol√≠tica para coleta de logs de autentica√ß√£o (auth.log).*

### 2. Visualiza√ß√£o de Ataques em Tempo Real
No Kibana, utilizei a funcionalidade **Discover** para filtrar eventos do dataset `system.auth` com resultado de falha (`event.outcome: failure`).

**Resultado:**
Os ataques de for√ßa bruta que antes eram apenas linhas de texto no terminal agora s√£o eventos estruturados no SIEM. O gr√°fico de volume mostra a persist√™ncia dos ataques ao longo do tempo.

![Discover SSH](images/34-kibana-discover-ssh-failures.png)
*Visualiza√ß√£o no Kibana confirmando a ingest√£o cont√≠nua de falhas de login SSH vindas da internet.*

---
