#  Building a SOC Home Lab: Detection & Response Project

Este projeto documenta a constru√ß√£o de um Laborat√≥rio de Security Operations Center (SOC) para simular ataques reais e praticar Defesa Cibern√©tica (Blue Team). O objetivo √© implementar uma stack completa de monitoramento (ELK), ingerir logs de endpoints e desenvolver habilidades de detec√ß√£o e resposta a incidentes.

## üìå Fase 1: Arquitetura e Infraestrutura 

### 1. Arquitetura L√≥gica
Antes do deploy, foi desenhada a topologia da rede para entender o fluxo de dados e posicionamento dos sensores. O laborat√≥rio simula um ambiente corporativo real contendo:

![Topologia de Rede](images/00-network-topology.png)
*Diagrama da arquitetura do laborat√≥rio desenhado durante o planejamento.*
* **VPC (Virtual Private Cloud):** Para isolamento da rede.
* **SIEM (ELK Stack):** O cora√ß√£o do monitoramento.
* **Endpoints (Windows/Linux):** Alvos que ser√£o monitorados e atacados.
* **C2 Server (Command & Control):** Para simular o atacante externo.

### 2. Provisionamento de Infraestrutura (IaaS)
Utilizei a **Vultr** como provedor de nuvem. A escolha foi baseada na capacidade de configurar redes privadas personalizadas e baixo custo.

**Configura√ß√µes do Servidor SIEM:**
* **OS:** Ubuntu 22.04 LTS (Estabilidade e suporte da comunidade).
* **Specs:** 2 vCPU, 8GB RAM (Dimensionado para suportar a JVM do Elasticsearch).
* **Regi√£o:** Toronto (Para garantir compatibilidade de lat√™ncia com a VPC).

![Status da VM e Specs](images/01-infrastructure-deploy.png)
*Deploy da inst√¢ncia Ubuntu Server conclu√≠do e rodando.*

### 3. Configura√ß√£o de Rede e Seguran√ßa (Hardening)
A seguran√ßa do pr√≥prio laborat√≥rio foi uma prioridade. N√£o expus o servidor diretamente sem prote√ß√µes.

* **VPC Network:** Criei uma rede privada isolada para que os agentes (futuros servidores Windows/Linux) se comuniquem com o SIEM sem trafegar logs sens√≠veis pela internet p√∫blica.
* **Firewall na Nuvem:** Implementei um *Firewall Group* na Vultr seguindo o princ√≠pio do menor privil√©gio.
    * **Porta 22 (SSH):** Restrita apenas ao meu IP atual de gerenciamento.
    * **Porta 9200 (Elasticsearch):** Restrita ao meu IP e √† rede interna da VPC.

![Regras de Firewall](images/02-firewall-hardening.png)
*Aplica√ß√£o de regras de firewall restringindo acesso SSH e TCP/9200.*

### 4. Instala√ß√£o e Configura√ß√£o do Elasticsearch
Realizei a instala√ß√£o manual do Elasticsearch via reposit√≥rio APT oficial, garantindo a vers√£o mais recente e segura.

**Credenciais e Seguran√ßa Inicial:**
Durante a instala√ß√£o, o Elasticsearch gerou automaticamente os tokens de seguran√ßa e a senha do superusu√°rio, garantindo que o banco de dados j√° nas√ßa autenticado.

![Instala√ß√£o e Senhas](images/03-elasticsearch-install.png)
*Output de seguran√ßa p√≥s-instala√ß√£o com credenciais geradas.*

**Ajustes de Configura√ß√£o (`elasticsearch.yml`):**
Para permitir que o servidor receba conex√µes externas (do meu laptop de analista e dos agentes), precisei alterar as configura√ß√µes de *binding* de rede, saindo do padr√£o `localhost`.

![Configura√ß√£o Nano](images/04-config-yaml.png)
*Edi√ß√£o do arquivo YAML definindo o host de rede p√∫blico e a porta padr√£o 9200.*

**Status do Servi√ßo:**
Ap√≥s a configura√ß√£o e recarga dos *daemons*, o servi√ßo subiu com sucesso e est√° ativo.

![Status Active](images/05-service-active.png)
*Valida√ß√£o do servi√ßo rodando via SystemD.*

---

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting)

Durante essa fase inicial, enfrentei desafios t√©cnicos reais de ambiente Cloud que exigiram adapta√ß√£o:

**1. Cota de Recursos na Cloud (Resource Quotas)**
Ao tentar provisionar a m√°quina de 16GB, fui bloqueada pela pol√≠tica de cota para novas contas ("Monthly Fee Limit Reached").

![Erro de Cota](images/99-troubleshooting-quota.png)
*Erro de limite de provisionamento encontrado.*

* **Solu√ß√£o:** Realizei o *downsizing* estrat√©gico para uma inst√¢ncia de 8GB RAM e otimizei o servidor. Isso permitiu continuar o laborat√≥rio dentro do or√ßamento e das limita√ß√µes da conta, sem perder funcionalidade cr√≠tica.

**3. IP Din√¢mico (CGNAT)**
Meu provedor de internet altera o IP frequentemente, o que bloqueava meu acesso √†s regras restritas do Firewall.
* **Solu√ß√£o:** Aprendi a monitorar meu IP p√∫blico e atualizar as regras de *Ingress* dinamicamente. Para testes de conectividade r√°pida, gerenciei o risco temporariamente via regras "Anywhere" combinadas com a autentica√ß√£o forte nativa do Elastic.

---

## üìå Fase 2: Visualiza√ß√£o de Dados com Kibana 

Com o backend de logs (Elasticsearch) funcional, o pr√≥ximo passo foi implementar o **Kibana**, a interface gr√°fica que permitir√° a visualiza√ß√£o dos dados, cria√ß√£o de dashboards e investiga√ß√£o de alertas de seguran√ßa.

### 1. Instala√ß√£o e Exposi√ß√£o do Servi√ßo
O Kibana foi instalado no mesmo servidor Ubuntu. Diferente do Elasticsearch (Backend), o Kibana precisa ser acess√≠vel via navegador.

**Configura√ß√£o de Rede (`kibana.yml`):**
Editei o arquivo de configura√ß√£o para alterar o `server.host`. Por padr√£o, ele vem travado em `localhost`. Configurei para ouvir no IP p√∫blico do servidor, permitindo o acesso remoto √† interface web na porta padr√£o `5601`.

![Configura√ß√£o Kibana](images/06-kibana-config-yaml.png)
*Ajuste do binding de rede para permitir acesso externo √† interface web.*

**Valida√ß√£o do Servi√ßo:**
Ap√≥s a configura√ß√£o, o servi√ßo foi iniciado e verificado via SystemD para garantir que n√£o houvesse erros de *bootstrap*.

![Status do Servi√ßo](images/07-kibana-service-status.png)
*Servi√ßo do Kibana ativo e rodando (Active/Running).*

### 2. Conex√£o Segura (Enrollment Token)
Para conectar o Kibana ao Elasticsearch de forma segura, utilizei o mecanismo de **Enrollment Tokens**. Isso garante que a comunica√ß√£o entre a interface e o banco de dados seja autenticada e criptografada, prevenindo intercepta√ß√£o de dados.

![Gera√ß√£o de Token](images/08-security-enrollment.png)
*Gera√ß√£o do token de inscri√ß√£o para pareamento seguro entre Kibana e Elasticsearch.*

### 3. Acesso e Configura√ß√£o de Criptografia (Keystore)
Ap√≥s o login inicial, obtive acesso √† interface central do Elastic ("Welcome Home"), confirmando que a stack ELK estava operacional.

![Elastic Home](images/10-elastic-home-welcome.png)
*Acesso bem-sucedido √† interface web do Elastic Stack.*

**Troubleshooting: Erro de Permiss√µes e Chaves de Criptografia**
Ao navegar para a aba de **Security > Alerts**, deparei-me com um erro de sistema: *"Detection engine permissions required"*.
Investigando a documenta√ß√£o, identifiquei que o erro n√£o era de permiss√µes de utilizador, mas sim a aus√™ncia de chaves de criptografia no *Keystore* do Kibana, necess√°rias para armazenar regras de alerta de forma segura.

![Erro Encryption](images/97-troubleshooting-encryption-error.png)
*Erro apresentado devido √† falta de chaves de criptografia persistentes.*

**Solu√ß√£o Aplicada:**
1.  Gerei novas chaves de criptografia via CLI (`kibana-encryption-keys generate`).
2.  Adicionei as chaves manualmente ao cofre seguro do Kibana (`kibana-keystore add`).
3.  Reiniciei o servi√ßo para aplicar as altera√ß√µes.

![Gerando Chaves](images/11-generating-encryption-keys.png)
*Gera√ß√£o e inser√ß√£o das chaves de seguran√ßa no Keystore.*

**Resultado Final:**
O painel de Alertas carregou com sucesso, pronto para receber dete√ß√µes de seguran√ßa.

![Alerts Corrigido](images/12-alerts-dashboard-fixed.png)
*Painel de Security Alerts totalmente operacional ap√≥s a corre√ß√£o.*

---

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting)

**1. Bloqueio de Firewall e Portas Reservadas**
Ao tentar acessar a interface web (`http://IP:5601`), recebi erros de *Connection Timed Out*. Diagnostiquei que o Firewall da Cloud (Vultr) estava bloqueando a porta 5601.
Ao tentar liberar o tr√°fego TCP, cometi um erro ao definir o range de portas iniciando em `0` (`0-65535`), o que foi rejeitado pela plataforma.

![Erro Firewall](images/98-troubleshooting-firewall-error.png)
*Erro ao tentar configurar regra de firewall com porta inv√°lida (0).*

* **Solu√ß√£o:** Ajustei a regra para um range v√°lido (`1-65535`) e configurei o acesso tempor√°rio para `Anywhere` (0.0.0.0/0) para fins de teste de conectividade, liberando o acesso ao painel do Kibana.

![Firewall Corrigido](images/09-firewall-fixed-kibana.png)
*Regra de firewall corrigida permitindo tr√°fego TCP na porta do Kibana.*

---

## üìå Fase 3: V√≠tima e Hardening 

Para simular um cen√°rio real de ataque, provisionei um servidor Windows Server 2022 exposto √† internet. Este servidor atuar√° como o *endpoint* monitorado e alvo das simula√ß√µes de ataque.

### 1. Arquitetura de Seguran√ßa (Isolamento)
Diferente dos componentes do SIEM (ELK), decidi **n√£o** conectar o servidor Windows √† VPC (Rede Privada).
* **Objetivo:** Garantir isolamento total (Network Segmentation). Caso o servidor Windows seja comprometido por um atacante real (o que √© esperado, dado que exporemos RDP), o atacante n√£o ter√° rota de rede lateral para alcan√ßar meu servidor de logs (Ubuntu/Elasticsearch).

![Specs Windows](images/13-windows-isolation-specs.png)
*Provisionamento do Windows Server 2022 fora da VPC para quarentena de rede.*

### 2. Acesso e Configura√ß√£o Inicial
O acesso inicial foi realizado via Console VNC (NoVNC) provido pela plataforma de nuvem para garantir que o sistema operacional completou o *boot* corretamente antes de expor servi√ßos de rede.

![Console Boot](images/14-windows-console-boot.png)
*Boot inicial e login administrativo via Console Web.*

### 3. Exposi√ß√£o Controlada (RDP)
Habilitei o protocolo RDP (Remote Desktop Protocol - Porta 3389) para administra√ß√£o remota.
* **Risco Aceito:** Manter o RDP exposto na internet √© uma vulnerabilidade cr√≠tica comum. Neste laborat√≥rio, isso √© intencional para gerar logs de *Brute Force* reais que ser√£o capturados e analisados pelo SIEM nas pr√≥ximas etapas.

![Acesso RDP](images/15-windows-rdp-access.png)
*Conex√£o remota bem-sucedida provando a acessibilidade p√∫blica do alvo.*

---
## üìå Fase 4: Ingest√£o de Dados e Fleet Server 

Com a infraestrutura do SIEM (ELK) e da V√≠tima (Windows) prontas, a pr√≥xima etapa foi conect√°-los. Para isso, utilizei a arquitetura **Elastic Fleet**, que centraliza o gerenciamento de todos os agentes de coleta.

Esta fase exigiu a cria√ß√£o de uma terceira VM Linux (`MyDFIR-Fleet-Server`) para atuar como o "Gerente" dos agentes, seguindo as boas pr√°ticas de separa√ß√£o de fun√ß√µes.

![Specs Fleet Server](images/16-fleet-server-deploy-specs.png)
*Provisionamento da VM dedicada para o Fleet Server.*

---

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting Avan√ßado)

Esta foi a fase mais complexa do projeto at√© o momento, apresentando m√∫ltiplos pontos de falha que exigiram diagn√≥stico em diferentes camadas (Rede, Aplica√ß√£o e Configura√ß√£o).

#### 1. Troubleshooting (Linux): Firewall e Conectividade
Ao tentar instalar o Fleet Server (agente Linux), a instala√ß√£o falhou com erros de `i/o timeout`.

![Erro de Conex√£o](images/17-troubleshoot-linux-firewall-error.png)
*Log de erro indicando que o Fleet Server n√£o conseguia se comunicar com o Elasticsearch na porta 9200.*

* **Diagn√≥stico:** O Firewall Group da Vultr, configurado para aceitar conex√µes apenas do "Meu IP", estava bloqueando a comunica√ß√£o interna entre os servidores (Fleet n√£o conseguia falar com ELK).
* **Solu√ß√£o:** Alterei a regra de firewall para `Anywhere (0.0.0.0/0)` para o range `1-65535`, permitindo a comunica√ß√£o interna necess√°ria para o laborat√≥rio. Isso resolveu o bloqueio das portas **9200** (Elastic) e **8220** (Fleet).

![Firewall Fix](images/18-vultr-firewall-fix.png)
*Ajuste nas regras de firewall para permitir a comunica√ß√£o interna do lab.*

Ap√≥s a corre√ß√£o do firewall, a instala√ß√£o do Fleet Server no Linux foi conclu√≠da com sucesso.

![Sucesso Linux](images/19-linux-agent-install-success.png)
*Instala√ß√£o do agente Fleet Server bem-sucedida.*

#### 2. Troubleshooting (Windows): Instala√ß√£o do Agente
A implanta√ß√£o do agente no Windows Server apresentou tr√™s erros em sequ√™ncia:
1.  **Erro de PowerShell:** O comando copiado do Kibana era longo e quebrava linhas, fazendo o PowerShell execut√°-lo incorretamente.
2.  **Erro de Caminho:** O PowerShell n√£o encontrava o `elastic-agent.exe` pois eu n√£o estava no diret√≥rio correto.
3.  **Erro de Loop (`:443`):** O agente instalava, mas entrava em loop infinito de conex√£o.

* **Solu√ß√£o (Comando Final):** Resolvi todos os problemas de uma vez construindo um comando de instala√ß√£o manual e robusto.
    1.  Naveguei para o diret√≥rio correto (`cd elastic-agent...`).
    2.  Usei o IP e a porta **correta** (`:8220`).
    3.  Adicionei a flag `--force` para sobrescrever a instala√ß√£o anterior falha.

![Sucesso Windows](images/20-windows-agent-install-success.png)
*Comando final no PowerShell (com `cd` e `--force`) que resultou na instala√ß√£o bem-sucedida.*

#### 3. Troubleshooting (Kibana): O Loop "Updating"
Ap√≥s a instala√ß√£o, o agente Windows ficou preso no status "Updating".
* **Diagn√≥stico:** Ao inspecionar a mensagem de erro no Kibana, notei que o agente tentava se comunicar na porta `:443`, apesar de eu ter for√ßado a instala√ß√£o na `:8220`. A **Pol√≠tica de Agente** (Agent Policy) no Kibana estava configurada com a URL errada, sobrescrevendo minha instala√ß√£o manual.

![Erro de Pol√≠tica](images/21-troubleshoot-kibana-policy-error.png)
*Configura√ß√£o do Fleet Server no Kibana apontando para a porta errada (443).*

* **Solu√ß√£o Definitiva:** Editei as configura√ß√µes do Fleet Server diretamente no Kibana, corrigindo a URL global para `https://[IP_DO_FLEET]:8220`. Ap√≥s reiniciar o servi√ßo no Windows (`Stop-Service/Start-Service`), o agente recebeu a pol√≠tica correta.

### 4. Valida√ß√£o Final da Infraestrutura
Com todas as corre√ß√µes aplicadas, ambos os agentes (Linux Fleet Server e Windows V√≠tima) reportaram status **Healthy** (Saud√°vel), confirmando que a infraestrutura de coleta de logs est√° 100% operacional.

![Dashboard Fleet](images/22-fleet-dashboard-all-healthy.png)
*Vis√£o final do painel Fleet com todos os agentes online e saud√°veis.*

---
## üìå Fase 5: Enriquecimento de Logs com Sysmon 

Com os agentes online, o pr√≥ximo passo foi enriquecer a qualidade dos dados coletados. O Elastic Agent coleta os logs de seguran√ßa padr√£o do Windows, mas para uma detec√ß√£o de amea√ßas eficaz (Threat Hunting), √© necess√°ria uma telemetria mais profunda.

Para isso, instalei o **Sysmon (System Monitor)** da Microsoft, a ferramenta padr√£o da ind√∫stria para monitoramento avan√ßado de *endpoints*.

### 1. Instala√ß√£o e Configura√ß√£o
A instala√ß√£o foi realizada no servidor Windows Server (V√≠tima). O ponto crucial foi n√£o instalar o Sysmon com as configura√ß√µes padr√£o (que s√£o muito "barulhentas").

Utilizei uma configura√ß√£o personalizada (`.xml`) baseada no projeto *SwiftOnSecurity*, que √© um padr√£o de mercado. Este arquivo filtra eventos de sistema irrelevantes e foca no que √© importante para a seguran√ßa, otimizando a ingest√£o de dados no SIEM.

![Instala√ß√£o Sysmon](images/23-sysmon-install-powershell.png)
*Instala√ß√£o do Sysmon via PowerShell (Admin), aplicando o arquivo de configura√ß√£o .xml.*

### 2. Valida√ß√£o Local
Ap√≥s a instala√ß√£o, validei que o Sysmon estava operacional na pr√≥pria m√°quina antes de tentar configur√°-lo no SIEM.

**1. Verifica√ß√£o do Servi√ßo:**
Confirmei que o servi√ßo `Sysmon64` foi instalado e estava em execu√ß√£o (`Running`) no `services.msc`.

![Servi√ßo Sysmon](images/24-sysmon-service-running.png)
*Servi√ßo Sysmon64 ativo e rodando em segundo plano.*

**2. Verifica√ß√£o dos Logs:**
Confirmei no **Visualizador de Eventos (Event Viewer)** que os logs estavam sendo gerados. Isso prova que o Sysmon est√° monitorando ativamente o sistema.

![Logs Sysmon Locais](images/25-sysmon-local-event-viewer.png)
*Logs operacionais do Sysmon (ex: Event ID 3 - Network connection) sendo gerados localmente.*

---
## üìå Fase 6: Ingest√£o de Logs no SIEM 

Com o Sysmon a gerar logs localmente, o passo final da infraestrutura foi configurar o *Elastic Agent* para ler esses arquivos e envi√°-los para o Elasticsearch.

### 1. Integra√ß√£o de Fontes de Dados (Data Ingestion)
No Kibana, configurei a pol√≠tica do agente Windows para incluir duas novas integra√ß√µes de **"Custom Windows Event Logs"**. Isso instrui o agente a ler canais espec√≠ficos do Windows Event Viewer.

**Canais Configurados:**
* **Sysmon:** `Microsoft-Windows-Sysmon/Operational` (Foco em cria√ß√£o de processos e rede).
* **Windows Defender:** `Microsoft-Windows-Windows Defender/Operational` (Foco em dete√ß√£o de malware).

![Config Sysmon](images/26-integration-sysmon-config.png)
*Configura√ß√£o do canal de ingest√£o para logs do Sysmon.*

![Config Defender](images/27-integration-defender-config.png)
*Configura√ß√£o do canal de ingest√£o para logs do Windows Defender.*

### 2. Valida√ß√£o de Recebimento (Data Discovery)
Ap√≥s aplicar a pol√≠tica, aguardei a propaga√ß√£o para o agente e validei o recebimento dos dados na aba **Discover** do Kibana.

Realizei testes gerando atividade no servidor (como reiniciar servi√ßos de seguran√ßa) para confirmar que os logs estavam a chegar quase em tempo real.

**Resultado:**
Os logs do Sysmon (ex: *Process Create*, Event ID 1) e do Defender come√ßaram a ser indexados corretamente pelo SIEM.

![Logs Sysmon](images/29-discover-sysmon-logs.png)
*Prova de ingest√£o: Log detalhado do Sysmon visualizado no Kibana.*

![Volume de Dados](images/30-discover-event-volume.png)
*Gr√°fico de volume de eventos confirmando o fluxo cont√≠nuo de dados entre a V√≠tima e o SIEM.*

---
## üìå Fase 7: Cria√ß√£o de Honeypot SSH e An√°lise de Ataques 

O objetivo desta fase era provisionar um servidor Linux exposto √† internet para atuar como "isca" (Honeypot) e capturar tentativas reais de ataque SSH (Brute Force).

### 1. Otimiza√ß√£o de Recursos (Engenharia)
Durante o provisionamento de uma quarta inst√¢ncia (Linux Target), atingi o limite de cota da conta de nuvem (Cloud Resource Quotas).

![Decis√£o de Recurso](images/31-resource-optimization-decision.png)
*Limite de inst√¢ncias atingido durante a tentativa de scale-out.*

* **Solu√ß√£o Arquitetural:** Em vez de solicitar aumento de cota (o que geraria custos), optei por reutilizar o servidor `MyDFIR-Fleet-Server`. Como ele j√° √© um servidor Linux Ubuntu exposto √† internet (necess√°rio para os agentes remotos), ele serve perfeitamente como alvo duplo: **Gerenciador de Agentes** e **Honeypot SSH**.

### 2. An√°lise de Logs de Autentica√ß√£o (`auth.log`)
Acessando o servidor via SSH, analisei os logs de autentica√ß√£o localizados em `/var/log/auth.log`.
Em pouco mais de 24 horas de exposi√ß√£o √† internet, o servidor registrou centenas de tentativas de acesso n√£o autorizado vindas de m√∫ltiplos endere√ßos IP globais.

**Evid√™ncias de Ataque:**
Os logs mostram bots tentando adivinhar senhas para usu√°rios comuns (`root`, `admin`) e servi√ßos espec√≠ficos (`git`, `composer`, `squid`).

![Logs de Ataque](images/32-ssh-bruteforce-evidence.png)
*Live logs demonstrando tentativas massivas de Brute Force contra o servidor exposto.*

---
## üìå Fase 8: Ingest√£o de Logs Linux e Monitoramento SSH 

Com o servidor Linux ("Honeypot") sob ataque constante, configurei o agente para coletar esses logs e envi√°-los para o SIEM, permitindo an√°lise centralizada.

### 1. Configura√ß√£o da Integra√ß√£o de Sistema
Como o *Fleet Server* j√° possu√≠a o Elastic Agent instalado, precisei apenas validar a pol√≠tica de agentes. Confirmei que a integra√ß√£o **System** estava ativa e configurada para ler os logs de autentica√ß√£o do sistema operacional.

* **Caminho do Log:** `/var/log/auth.log` (Padr√£o Ubuntu/Debian).
* **Dataset:** `system.auth`.

![Configura√ß√£o Linux](images/33-linux-system-integration-config.png)
*Configura√ß√£o da pol√≠tica para coleta de logs de autentica√ß√£o (auth.log).*

### 2. Visualiza√ß√£o de Ataques em Tempo Real
No Kibana, utilizei a funcionalidade **Discover** para filtrar eventos do dataset `system.auth` com resultado de falha (`event.outcome: failure`).

**Resultado:**
Os ataques de for√ßa bruta que antes eram apenas linhas de texto no terminal agora s√£o eventos estruturados no SIEM. O gr√°fico de volume mostra a persist√™ncia dos ataques ao longo do tempo.

![Discover SSH](images/34-kibana-discover-ssh-failures.png)
*Visualiza√ß√£o no Kibana confirmando a ingest√£o cont√≠nua de falhas de login SSH vindas da internet.*

---
## üìå Fase 9: Cria√ß√£o de Alertas e Dashboards 

Com os dados de ataque fluindo para o SIEM, o objetivo final era transformar logs brutos em intelig√™ncia acion√°vel. Criei mecanismos de detec√ß√£o autom√°tica e visualiza√ß√£o geogr√°fica.

### 1. Regra de Detec√ß√£o (Alerting)
Criei uma regra de alerta para detectar padr√µes de for√ßa bruta (Brute Force).
* **L√≥gica:** Se um √∫nico host gerar mais de **5 falhas de autentica√ß√£o SSH** (`system.auth.ssh.event: Failed`) em um intervalo de **5 minutos**, um alerta de severidade m√©dia √© disparado.

![Regra de Alerta](images/37-alert-rule-threshold-config.png)
*Configura√ß√£o da regra de threshold para detec√ß√£o de for√ßa bruta SSH.*

### 2. Constru√ß√£o de Dashboards (Threat Intelligence)
Para visualizar a origem dos ataques, utilizei o **Elastic Maps**.
Configurei uma camada (*Layer*) baseada no campo `source.geo.country_iso_code`, que traduz o IP do atacante em sua localiza√ß√£o geogr√°fica.

![Config Mapa](images/38-map-layer-iso-code.png)
*Mapeamento de IPs para geolocaliza√ß√£o usando c√≥digos ISO de pa√≠ses.*

### 3. Resultado Final: O Mapa de Amea√ßas
O Dashboard final apresenta uma vis√£o em tempo real da ciberseguran√ßa do servidor. Em poucas horas de monitoramento, foi poss√≠vel identificar ataques distribu√≠dos vindos da Europa e √Åsia (Fran√ßa, Indon√©sia, It√°lia), confirmando a natureza global das amea√ßas automatizadas.

![Mapa de Ataques](images/39-final-dashboard-threat-map.png)
*Dashboard de Threat Hunting visualizando a origem global dos ataques SSH.*

---
## üìå Fase 10: Engenharia de Detec√ß√£o e SIEM 

Com a ingest√£o de dados validada, avancei para a cria√ß√£o de regras de detec√ß√£o (Detection Engineering) utilizando o m√≥dulo **Elastic Security**. Diferente dos alertas simples, estas regras utilizam l√≥gica de correla√ß√£o e *thresholds* (limiares) para identificar comportamentos an√¥malos.

### 1. An√°lise de Padr√µes de Ataque RDP
Antes de criar a regra, analisei os logs brutos do Windows no Discover. Identifiquei que falhas de login geram o **Event ID 4625** (*An account failed to log on*). Este ID √© a base para detectar tentativas de acesso n√£o autorizado.

![Logs RDP 4625](images/40-discover-rdp-failure-logs.png)
*Identifica√ß√£o de logs de falha de autentica√ß√£o Windows (Event ID 4625) para basear a regra de detec√ß√£o.*

### 2. Cria√ß√£o de Regras de Detec√ß√£o (Threshold Rules)
Criei duas regras distintas no SIEM, uma para cada sistema operacional, garantindo precis√£o e evitando falsos positivos.

**Regra 1: SSH Brute Force (Linux)**
* **Alvo:** `MyDFIR-Fleet-Server`
* **Query:** `system.auth.ssh.event: *` (Eventos de autentica√ß√£o SSH).
* **L√≥gica:** Dispara se houver **5 ou mais** tentativas de falha vindas do mesmo IP para o mesmo usu√°rio em 5 minutos.

![L√≥gica SSH](images/41-security-rule-ssh-logic.png)
*Configura√ß√£o da regra de threshold para Linux, agrupando eventos por IP de origem e usu√°rio.*

**Regra 2: RDP Brute Force (Windows)**
* **Alvo:** `Win-Server-Lab`
* **Query:** `event.code: 4625` (Logon Failure).
* **L√≥gica:** Similar √† do Linux, detecta volume alto de erros de senha via RDP.

![L√≥gica RDP](images/42-security-rule-rdp-logic.png)
*Configura√ß√£o da regra de threshold para Windows, focada no Event ID 4625.*

### 3. Status do Monitoramento
As regras foram ativadas e est√£o monitorando em tempo real. Qualquer atividade que ultrapasse os limiares definidos gerar√° automaticamente um "Alerta de Seguran√ßa" no painel do analista.

![Regras Ativas](images/43-active-detection-rules-list.png)
*Painel de Detection Rules com as regras de Linux e Windows implementadas e ativas.*

---
## üìå Fase 11: Visualiza√ß√£o de Amea√ßas RDP e Dashboard Unificado 

Para complementar a visibilidade, expandi o Dashboard para incluir as tentativas de ataque ao servidor Windows (RDP). O objetivo final foi criar um "Painel de Controle" (Single Pane of Glass) que unificasse a vis√£o de amea√ßas de toda a infraestrutura.

### 1. Mapeamento de Ataques Windows
Configurei um novo mapa no Kibana filtrando especificamente pelo **Event ID 4625** (Falha de Login RDP). Isso permite visualizar geograficamente de onde v√™m as tentativas de invas√£o ao servidor Windows, separando-as visualmente dos ataques SSH.

![Mapa RDP](images/44-rdp-map-layer-config.png)
*Camada de mapa configurada para plotar tentativas de acesso RDP por geolocaliza√ß√£o.*

### 2. An√°lise Tabular (Top Offenders)
Al√©m do mapa, criei visualiza√ß√µes em tabela para identificar os principais ofensores.
As tabelas agregam dados por:
* **User Name:** Quais usu√°rios est√£o sendo mais testados (ex: `Administrator`, `root`, `admin`).
* **Source IP:** Quais endere√ßos IP est√£o gerando mais volume de ataque.
* **Pa√≠s de Origem:** Vis√£o consolidada por na√ß√£o.

![Tabela de Atacantes](images/45-attacker-table-visualization.png)
*Tabela din√¢mica classificando os top 10 IPs e usu√°rios utilizados nas tentativas de for√ßa bruta.*

### 3. Dashboard Unificado de Amea√ßas
Consolidei todas as visualiza√ß√µes em um √∫nico Dashboard Operacional, totalizando 8 pain√©is de visualiza√ß√£o divididos por sistema operacional:

* **Se√ß√£o Superior (Linux/SSH):** Mapas de geolocaliza√ß√£o e tabelas de origem de ataques ao Fleet Server.
* **Se√ß√£o Inferior (Windows/RDP):** Mapas e tabelas focados nas tentativas de intrus√£o ao servidor Windows.

Esta organiza√ß√£o permite uma leitura vertical r√°pida e correlacionada de toda a superf√≠cie de ataque.

![Dashboard Final](images/46-unified-threat-dashboard.png)
*Vis√£o geral do Dashboard de Seguran√ßa no Kibana.*

![Dashboard Completo Expandido](images/47-unified-threat-dashboard2.png)
*Vis√£o detalhada (Full Page) incluindo as tabelas de "Top Offenders" para ambos os protocolos.*

---
## üìå Fase 12: Planejamento de Ataque e C2 

Antes de executar a simula√ß√£o de advers√°rio, desenhei o fluxo de ataque (Kill Chain) para garantir que todos os est√°gios gerem telemetria detect√°vel pelo SIEM. O plano segue a estrutura do framework MITRE ATT&CK.

**O Plano de Ataque:**

**Parte 1: Acesso e Reconhecimento**
1.  **Initial Access:** For√ßa bruta via RDP para ganhar acesso ao servidor.
2.  **Discovery:** Execu√ß√£o de comandos de descoberta (`whoami`, `ipconfig`, `net user`) para mapear o ambiente.
3.  **Defense Evasion:** Desabilita√ß√£o manual do Windows Defender para permitir a execu√ß√£o do malware.

![Diagrama Parte 1](images/48-attack-diagram-part1.png)
*Fases iniciais do ataque planejado: Acesso, Descoberta e Evas√£o.*

**Parte 2: Comando e Controle**
4.  **Execution:** Download e execu√ß√£o do agente malicioso via PowerShell.
5.  **Command & Control (C2):** Estabelecimento de comunica√ß√£o persistente com o servidor Mythic C2.
6.  **Exfiltration:** Simula√ß√£o de roubo de dados (arquivo `passwords.txt`).

![Diagrama Parte 2](images/49-attack-diagram-part2.png)
*Fases finais do ataque: Execu√ß√£o de C2 e Exfiltra√ß√£o de dados.*

---
## üìå Fase 13: Infraestrutura de Comando e Controle (C2) 

Para executar o ataque planejado, precisei de uma infraestrutura de **C2 (Command & Control)**. A ferramenta escolhida foi o **Mythic**, um framework C2 moderno, multi-usu√°rio e baseado em Docker, amplamente utilizado em opera√ß√µes de Red Team.

### 1. Instala√ß√£o do Servidor C2
Devido √†s restri√ß√µes de cota na nuvem, realizei a instala√ß√£o do Mythic no servidor `MyDFIR-Fleet-Server`, aproveitando os recursos dispon√≠veis (4GB RAM) para rodar os containers Docker necess√°rios.

* **Deploy:** Instala√ß√£o de depend√™ncias (Docker Compose) e compila√ß√£o dos servi√ßos via `make`.
* **Rede:** Configura√ß√£o de regra de Firewall para expor a porta administrativa `7443`.

![Docker Setup](images/50-mythic-prerequisites-docker.png)
*Prepara√ß√£o do ambiente e instala√ß√£o do Docker Compose.*

![Build Mythic](images/51-mythic-build-process.png)
*Compila√ß√£o dos containers do framework Mythic.*

### 2. Configura√ß√£o e Acesso
Ap√≥s o build, recuperei as credenciais de administra√ß√£o geradas no arquivo de ambiente (`.env`) e validei o acesso ao painel de opera√ß√µes.

![Credenciais](images/53-mythic-credentials-env.png)
*Recupera√ß√£o segura das credenciais de acesso administrativo.*

![Dashboard C2](images/54-mythic-c2-dashboard-active.png)
*Painel de Opera√ß√µes do Mythic C2 online e pronto para gerenciar agentes.*

### 3. M√°quina de Ataque (Kali Linux)
Paralelamente, configurei uma m√°quina virtual local com **Kali Linux** utilizando VirtualBox. Esta m√°quina servir√° como o ponto de lan√ßamento dos ataques manuais e gera√ß√£o de payloads, conectando-se ao C2 na nuvem.

![Setup Kali](images/55-kali-linux-local-setup.png)
*Virtualiza√ß√£o local do Kali Linux para opera√ß√µes ofensivas.*

---
## üìå Fase 14: Execu√ß√£o de Ataque - Weaponization e Initial Access 

Nesta fase cr√≠tica, executei o ciclo completo de ataque (Kill Chain), desde a prepara√ß√£o do artefato malicioso at√© o acesso inicial via for√ßa bruta.

### 1. Weaponization (Cria√ß√£o do Payload)
Utilizei o servidor Mythic C2 para gerar um agente malicioso (**Apollo**) configurado para sistemas Windows.
* **Perfil:** HTTP (comunica√ß√£o via porta 80/443 simulada).
* **Formato:** Execut√°vel Windows (`svchost.exe`) para evadir detec√ß√£o simples por nome.

![Instala√ß√£o Apollo](images/56-mythic-apollo-agent-install.png)
*Instala√ß√£o do agente Apollo no servidor C2.*

![Configura√ß√£o Payload](images/57-payload-configuration-ui.png)
*Configura√ß√£o do payload HTTP para comunica√ß√£o persistente com o C2.*

### 2. Initial Access (Ataque de For√ßa Bruta)
Para entregar o payload, precisei primeiro ganhar acesso ao servidor. Utilizei o **Kali Linux** para executar um ataque de dicion√°rio contra o servi√ßo RDP.

**Ferramentas:** `Hydra` e `xFreeRDP`.
**T√©cnica:** T1110 (Brute Force).

Ap√≥s ajustar as configura√ß√µes de NLA (Network Level Authentication) no alvo para permitir conex√µes legadas, o Hydra recuperou com sucesso a senha de Administrador.

![Hydra Sucesso](images/60-hydra-rdp-success.png)
*Execu√ß√£o bem-sucedida do Hydra recuperando credenciais de acesso.*

![Acesso Confirmado](images/61-xfreerdp-access-confirmed.png)
*Acesso RDP obtido via Kali Linux utilizando as credenciais comprometidas.*

### 3. Command & Control (Callback)
Com acesso ao servidor, transferi e executei o payload. O agente Apollo estabeleceu conex√£o imediata com o servidor Mythic, concedendo controle remoto total sobre a v√≠tima.

![Callback C2](images/58-mythic-c2-successful-callback.png)
*Sess√£o ativa no Mythic C2, confirmando o comprometimento total do servidor Windows.*

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting Ofensivo)

Durante a execu√ß√£o do ataque, enfrentei mecanismos de defesa nativos do Windows e problemas de conectividade que exigiram adapta√ß√£o das t√°ticas.

#### 1. Bloqueio de Conex√£o RDP (NLA)
Ao tentar executar o Hydra, recebi erros de `[ERROR] freerdp: The connection failed to establish`, mesmo com o servidor online.
* **Diagn√≥stico:** O alvo estava configurado com **NLA (Network Level Authentication)** ativo, que rejeita conex√µes de ferramentas de for√ßa bruta legadas antes mesmo da tentativa de senha.
* **Solu√ß√£o:** Desativei o NLA no servidor alvo via GUI e garanti a altera√ß√£o via registro do Windows para permitir a negocia√ß√£o de credenciais pelo Hydra.

#### 2. Falhas no Brute Force (Rate Limiting)
Mesmo com a senha correta na wordlist, o Hydra falhava em identificar o sucesso ou perdia a conex√£o.
* **Diagn√≥stico:** O envio padr√£o de m√∫ltiplas threads paralelas sobrecarregava o servi√ßo RDP, causando nega√ß√£o de servi√ßo tempor√°ria ou bloqueio.
* **Solu√ß√£o:** Ajustei os par√¢metros do ataque para ser mais lento e sequencial (`-t 1` para uma task por vez e `-W 3` para espera entre tentativas), garantindo estabilidade na conex√£o.

#### 3. Execu√ß√£o do Payload (Caminhos e Sintaxe)
Tive dificuldades ao executar o payload via PowerShell devido a erros de *Path* e sintaxe de comandos de download (`Invoke-WebRequest`).
* **Solu√ß√£o:** Optei pelo download direto via navegador para garantir a integridade do arquivo e executei o artefato malicioso (`svchost.exe`) navegando manualmente at√© o diret√≥rio de usu√°rio, contornando erros de caminho relativo.
---
## üö® Incidente Real: Cloud Abuse Report & Remedia√ß√£o

Durante a execu√ß√£o da simula√ß√£o de C2 (Mythic), a infraestrutura do laborat√≥rio foi detectada por scanners de Threat Intelligence externos (Spamhaus), gerando um reporte de abuso real junto ao provedor de nuvem (Vultr).

**O Evento:**
* **Detec√ß√£o:** Atividade de "Botnet C2" na porta 7443/80.
* **Causa Raiz:** Falha de OPSEC (Operational Security). As regras de firewall foram configuradas como `Anywhere (0.0.0.0/0)` para facilitar a conectividade do laborat√≥rio, expondo a assinatura do C2 √† internet p√∫blica.

**A√ß√£o de Resposta (Containment & Eradication):**
1.  **Isolamento:** O servidor comprometido/ofensor (`MyDFIR-Fleet-Server`) foi imediatamente destru√≠do para cessar a exposi√ß√£o.
2.  **Comunica√ß√£o:** Resposta formal ao time de Trust & Safety do provedor, detalhando o contexto educacional e as medidas de corre√ß√£o tomadas.
3.  **Li√ß√µes Aprendidas:** Em implementa√ß√µes futuras de C2, o acesso deve ser estritamente restrito via *Allowlisting* de IPs (apenas meu IP residencial) ou via VPN, nunca exposto publicamente.
---
## üìå Fase 15: Implementa√ß√£o de Sistema de Ticketing 

Ap√≥s o incidente de seguran√ßa simulado (e o real com a nuvem), ficou clara a necessidade de uma plataforma para gerenciar, triar e documentar os incidentes de forma organizada.

Para isso, implementei o **osTicket**, um sistema de Help Desk open-source amplamente utilizado para gest√£o de chamados em TI e Seguran√ßa.

### 1. Provisionamento e Hardening
Criei um novo servidor Windows Server 2022 para hospedar a aplica√ß√£o web (PHP/MySQL).
Aplicando as li√ß√µes de OPSEC aprendidas anteriormente, implementei uma pol√≠tica de firewall restritiva desde o in√≠cio:
* **Porta 80 (HTTP):** Acesso permitido *apenas* para meu endere√ßo IP de gerenciamento.
* **Porta 3389 (RDP):** Acesso restrito ao meu IP.

![Firewall Seguro](images/62-osticket-firewall-hardening.png)
*Regras de firewall configuradas com Allowlisting estrito para prevenir exposi√ß√£o p√∫blica indesejada.*

### 2. Instala√ß√£o da Stack Web (WAMP)
Configurei o ambiente de servidor web utilizando o pacote XAMPP (Apache, MySQL, PHP) e realizei o deploy da aplica√ß√£o osTicket, criando o banco de dados e definindo as credenciais administrativas.

### 3. Valida√ß√£o do Sistema
O sistema est√° operacional e pronto para receber integra√ß√µes via API. A partir de agora, os alertas gerados no ELK (como o Brute Force RDP) poder√£o ser transformados automaticamente em tickets para investiga√ß√£o formal.

![Painel osTicket](images/63-osticket-dashboard-success.png)
*Painel administrativo do osTicket funcional e pronto para opera√ß√£o.*

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting de Infraestrutura)

Durante o deploy do servidor de gest√£o, enfrentei desafios relacionados com performance e seguran√ßa de rede.

#### 1. Gargalo de Recursos (Resource Contention)
Ao instalar a stack WAMP (XAMPP) no Windows Server, o sistema tornou-se n√£o responsivo, travando a sess√£o RDP e o cursor do rato.
* **Diagn√≥stico:** A inst√¢ncia selecionada (1 vCPU / 2GB RAM) atingiu 100% de uso de CPU ao tentar processar a descompacta√ß√£o do instalador simultaneamente com o scan em tempo real do Windows Defender.
* **Solu√ß√£o:** Apliquei t√©cnicas de gest√£o de sess√£o (reconex√£o RDP para limpar o buffer de v√≠deo) e aguardei a finaliza√ß√£o dos processos de I/O intensivos, evitando rein√≠cios for√ßados que poderiam corromper a instala√ß√£o do banco de dados.

#### 2. Corre√ß√£o de Vulnerabilidade de Firewall
Identifiquei que o Firewall Group herdado dos laborat√≥rios anteriores possu√≠a uma regra residual de "Allow All" (`0.0.0.0/0` em todas as portas TCP), o que gerou o incidente de abuso anterior.
* **Solu√ß√£o (Hardening):** Realizei uma auditoria nas regras e removi as permiss√µes gen√©ricas. Configurei o acesso √†s portas administrativas (3389 e 80) estritamente para o meu endere√ßo IP (`My IP/32`), garantindo que o painel de gest√£o do osTicket n√£o fique exposto a scanners p√∫blicos.

#### 3. Erro de Permiss√£o MySQL e Revers√£o de Configura√ß√£o
Ao alterar o *host* do banco de dados para o IP P√∫blico no arquivo `config.inc.php`, perdi o acesso ao PHPMyAdmin com o erro *"Host is not allowed to connect"*.
* **Diagn√≥stico:** O usu√°rio `root` do banco de dados estava configurado para aceitar conex√µes apenas de `localhost`. Ao mudar a configura√ß√£o do arquivo antes de alterar as permiss√µes do usu√°rio, quebrei a conectividade.
* **Solu√ß√£o:** Reverti a configura√ß√£o para o padr√£o (usando o backup ou edi√ß√£o manual), acessei o painel localmente, concedi permiss√µes expl√≠citas para o meu IP P√∫blico nos usu√°rios do banco e, somente ent√£o, reapliquei a configura√ß√£o de rede externa..
---
## üìå Fase 16: Automa√ß√£o de Resposta a Incidentes - SOAR 

Com o sistema de gest√£o operacional (osTicket) no ar, o objetivo final foi criar uma automa√ß√£o **SOAR (Security Orchestration, Automation, and Response)**. Configurei o ELK Stack para se comunicar automaticamente com o osTicket via API, fechando o ciclo de detec√ß√£o e resposta.

### 1. Configura√ß√£o da API e Conector
* **No osTicket:** Criei uma API Key vinculada estritamente ao IP do servidor ELK (`155.138...`), autorizando apenas este servidor a abrir chamados.
* **No Kibana:** Configurei um conector do tipo **Webhook**. Como o osTicket exige autentica√ß√£o via cabe√ßalho, utilizei a funcionalidade de *Custom HTTP Headers* para injetar a chave `X-API-Key` nas requisi√ß√µes.

![API Key](images/64-osticket-api-key-config.png)
*Configura√ß√£o de seguran√ßa da API Key, limitando o acesso ao IP de origem do SIEM.*

![Webhook Config](images/65-kibana-connector-webhook.png)
*Configura√ß√£o do conector Webhook utilizando autentica√ß√£o via Custom Header.*

### 2. Defini√ß√£o do Payload
Configurei o corpo da requisi√ß√£o (Payload) em formato XML, mapeando os campos do alerta do Elastic para os campos do ticket (Assunto, Mensagem, Prioridade).

![Payload XML](images/66-connector-payload-xml.png)
*Estrutura XML definida para a cria√ß√£o autom√°tica do ticket.*

### 3. Valida√ß√£o da Automa√ß√£o
Realizei testes de conectividade enviando dados simulados.
* **Resultado:** O conector obteve resposta `201 Created`, e o ticket apareceu instantaneamente no painel de suporte, validando a integra√ß√£o end-to-end.

![Ticket Criado](images/67-automated-ticket-created.png)
*Ticket gerado automaticamente no osTicket via gatilho do SIEM.*

---

### ‚ö†Ô∏è Desafios e Solu√ß√µes (Troubleshooting Cr√≠tico de Integra√ß√£o)

Esta fase apresentou erros cr√≠ticos que exigiram reconstru√ß√£o de infraestrutura e recupera√ß√£o de banco de dados.

#### 1. Corrup√ß√£o de Banco de Dados (MySQL InnoDB)
Ap√≥s um rein√≠cio for√ßado do servidor (devido a travamento por falta de recursos), o servi√ßo MySQL falhou ao iniciar. Logs indicaram corrup√ß√£o nos arquivos de transa√ß√£o (`ib_logfile`).
* **Diagn√≥stico:** O desligamento abrupto corrompeu a integridade do InnoDB, impedindo o *startup* do banco e o login na aplica√ß√£o (`Authentication Required`).
* **Solu√ß√£o:** Tentei a recupera√ß√£o via limpeza de logs tempor√°rios, mas a corrup√ß√£o era estrutural. Optei por realizar uma **reinstala√ß√£o limpa (Clean Re-install)** da stack XAMPP e recria√ß√£o do banco de dados `osticket_db`, restaurando o servi√ßo em menos tempo do que uma recupera√ß√£o forense de banco exigiria.

#### 2. Conflito de Instala√ß√£o (Diret√≥rio Sujo)
Durante a reinstala√ß√£o, o instalador falhou devido a res√≠duos da instala√ß√£o anterior.
* **Solu√ß√£o:** Realizei a limpeza manual do diret√≥rio `C:\xampp` e forcei a remo√ß√£o de processos travados para garantir um *deploy* limpo.

#### 3. Erro de Autentica√ß√£o API (403/401)
Inicialmente, os testes de conex√£o falhavam.
* **Diagn√≥stico:** Identifiquei que o conector do Elastic n√£o enviava a API Key no formato esperado pela autentica√ß√£o b√°sica.
* **Solu√ß√£o:** Configurei manualmente o *Header* HTTP `X-API-Key` no conector do Kibana, garantindo que a credencial fosse passada corretamente para o gateway do osTicket.

---

## üìå Fase 17: Investiga√ß√£o de Incidente e Threat Hunting 

Ao receber o alerta de **SSH Brute Force**, iniciei o processo manual de investiga√ß√£o para determinar a extens√£o e o impacto do ataque. Segui o *playbook* de resposta a incidentes para responder a quatro perguntas cr√≠ticas:

### 1. Investiga√ß√£o do Atacante (Threat Intelligence)
**P:** Este IP √© conhecido por atividades maliciosas?
**R:** Sim.
* **Evid√™ncia:** Consultei o IP `77.83.207.205` no **AbuseIPDB**. O endere√ßo possui "Confidence of Abuse: 100%", sendo reportado centenas de vezes por ataques de for√ßa bruta globalmente.

![Intel Externa](images/70-threat-intel-abuseipdb.png)
*Consulta de reputa√ß√£o confirmando a origem maliciosa do IP.*

### 2. An√°lise de Escopo (Scope Analysis)
**P:** Outros usu√°rios foram afetados al√©m do `root`?
**R:** Sim.
* **Evid√™ncia:** Filtrando os logs no Kibana Discover pelo IP do atacante, identifiquei tentativas de login para diversos usu√°rios gen√©ricos, incluindo `admin`, `test`, `user`, `guest`. Isso indica um ataque de dicion√°rio amplo (spray), e n√£o um ataque direcionado a uma credencial espec√≠fica.

![Logs Brutos](images/69-discover-threat-analysis.png)
*An√°lise de logs no Discover revelando o padr√£o de tentativa de m√∫ltiplos usu√°rios.*

### 3. Avalia√ß√£o de Impacto (Containment)
**P:** Alguma tentativa obteve sucesso?
**R:** **N√£o.**
* **Metodologia:** Realizei uma query de busca por `event.outcome: "success"` filtrando pelo IP do atacante. O retorno foi de **0 eventos**.

**P:** Houve atividade p√≥s-explora√ß√£o?
**R:** **N/A.** Como n√£o houve sucesso no login, n√£o houve execu√ß√£o de comandos ou movimenta√ß√£o lateral.

---
**Conclus√£o da An√°lise:** Tentativa de acesso n√£o autorizado falha. O bloqueio de firewall e senhas fortes foram eficazes. Incidente classificado como **Tentativa de Intrus√£o (N√≠vel Baixo/Monitoramento)**.
---
## üìå Fase 18: Investiga√ß√£o Profunda e Encerramento

Ap√≥s validar a tentativa de for√ßa bruta, aprofundei a investiga√ß√£o para responder √† pergunta cr√≠tica: **"O ataque obteve sucesso?"**

### 1. Ca√ßa ao Sucesso (Hunting for Success)
Filtrei os logs no SIEM buscando pelo **Event ID 4624** (Login Sucesso) correlacionado com o IP do atacante identificado anteriormente.
* **Resultado:** Localizei m√∫ltiplos eventos de sucesso √†s 12:37, coincidindo com o fim da tentativa de for√ßa bruta.
* **Significado:** O atacante conseguiu descobrir a senha e comprometer a conta `Administrator`.

![Sucesso Confirmado](images/72-investigation-success-found.png)
*Identifica√ß√£o visual no Elastic: picos de eventos 4624 originados pelo IP do atacante.*

### 2. An√°lise da Evid√™ncia
Ao expandir os logs, confirmei os detalhes da intrus√£o. O sistema registrou o **Logon Type 10** (Acesso Remoto/RDP) ou **Logon Type 3** (Rede), validando que a credencial foi usada externamente.

![Detalhes do Log](images/73-evidence-log-details.png)
*Detalhes do evento mostrando o acesso bem-sucedido √† conta de Administrador.*

### 3. Conten√ß√£o e Documenta√ß√£o
Com a confirma√ß√£o da invas√£o, iniciei o protocolo de resposta a incidentes:
1.  **Cria√ß√£o de Ticket:** Registrei o incidente no **osTicket** detalhando a detec√ß√£o.
2.  **Mitiga√ß√£o:** (Simulado) Reset da senha de Administrador e bloqueio do IP no Firewall.
3.  **Conclus√£o:** O ticket foi atualizado com as evid√™ncias e marcado como "Fechado".

![Cria√ß√£o do Ticket](images/74-incident-ticket-creation.png)
*Registro formal do incidente no sistema de tickets para rastreabilidade.*

---
**STATUS ATUAL DO PROJETO:**
O ciclo manual de ataque e defesa foi conclu√≠do.
* **Ataque:** Realizado (Brute Force).
* **Detec√ß√£o:** Confirmada (Elastic SIEM).
* **Gest√£o:** Documentada (osTicket).

![Fila de Tickets](images/75-ticket-queue-status.png)
*Vis√£o da fila de tickets demonstrando o fluxo de trabalho do analista.*

üìå Fase 19: Implementa√ß√£o de SOAR (Security Orchestration, Automation and Response)
Para reduzir o tempo de resposta (MTTR), integrei o Elastic SIEM a uma plataforma SOAR (Tines). O objetivo foi automatizar a notifica√ß√£o de alertas cr√≠ticos, eliminando a necessidade de monitoramento visual constante.

1. Constru√ß√£o do Storyboard
Criei um fluxo de automa√ß√£o composto por tr√™s est√°gios principais:

Webhook: Recebimento do alerta enviado pelo Elastic.

Data Parsing: Tratamento do JSON bruto para extrair campos vitais (Nome da Regra, Host, Usu√°rio, Comando Malicioso).

Action (Email): Envio din√¢mico de notifica√ß√£o para o analista.

Arquitetura da automa√ß√£o no Tines conectando o SIEM ao sistema de notifica√ß√£o.

üìå Fase 20: Simula√ß√£o de Amea√ßa Avan√ßada (PowerShell/C2)
Diferente do ataque de for√ßa bruta (barulhento), simulei uma t√©cnica mais furtiva e comum em est√°gios de p√≥s-explora√ß√£o: a execu√ß√£o de comandos codificados em Base64 via PowerShell (T1059.001 no MITRE ATT&CK).

1. O Ataque
Utilizei um payload codificado para ofuscar o comando malicioso, tentando evadir detec√ß√µes baseadas em assinaturas simples de texto.

PowerShell

powershell.exe -EncodedCommand JABzACAAPQAgAE4AZQB3AC0ATwBiAGoAZQBjAHQAIABJAE8ALgBNAGUAbQBvAHIAeQBTAHQAcgBlAGEAbQAoAFsAQwBvAG4AdgBlAHIAdABdADoAOgBGAH...
(Nota: O comando real foi executado no ambiente controlado).

2. A Detec√ß√£o (Regra Customizada)
Configurei uma regra de detec√ß√£o no Elastic baseada na query process.command_line: *EncodedCommand*. A regra identificou a anomalia imediatamente ap√≥s a execu√ß√£o.

SIEM detectando a execu√ß√£o do processo suspeito via command line.

üìå Fase 21: Valida√ß√£o do Ciclo Completo (End-to-End)
O teste final consistiu em disparar o ataque e verificar se a automa√ß√£o funcionaria sem interven√ß√£o humana.

1. Resultado da Automa√ß√£o
Segundos ap√≥s a detec√ß√£o no Elastic, o Tines processou o evento e enviou um e-mail formatado contendo os detalhes cr√≠ticos do incidente. Isso prova a capacidade de resposta em tempo real.

E-mail recebido automaticamente contendo a regra disparada, o usu√°rio e o comando malicioso.

üìå Fase 22: Reporting e Encerramento
Para finalizar o laborat√≥rio e garantir a preserva√ß√£o das evid√™ncias forenses, gerei relat√≥rios dos incidentes confirmados.

1. Exporta√ß√£o de Dados
Filtrei os logs no Discover para isolar apenas os eventos de alta fidelidade (PowerShell Encoded e Sucesso de Login) e exportei os dados em formato CSV para auditoria futura.

Filtro aplicado no Discover para exporta√ß√£o das evid√™ncias finais.

üèÅ Conclus√£o do Projeto
O laborat√≥rio demonstrou com sucesso a cria√ß√£o de um ecossistema de seguran√ßa defensiva funcional, cobrindo:

Ingest√£o de Logs: Windows e Linux enviando telemetria para a nuvem.

Visibilidade: Dashboards e Mapas em tempo real.

Detec√ß√£o: Regras para Brute Force e Execu√ß√£o de Processos.

Resposta: Automa√ß√£o via SOAR para alertas imediatos.
